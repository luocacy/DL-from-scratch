{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 卷积层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im2col\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride = 1, pad = 0)\n",
    "print(col1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride = 1, pad = 0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#卷积层的前向传播\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b \n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T    #将各个滤波器纵向展开为1列\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "                     \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)    #transpose()改变轴的顺序\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 池化层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 展开 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 最大值 (2)\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        # 转换 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    \"\"\"简单的ConvNet\n",
    "\n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 输入大小（MNIST的情况下为784）\n",
    "    hidden_size_list : 隐藏层的神经元数量的列表（e.g. [100, 100, 100]）\n",
    "    output_size : 输出大小（MNIST的情况下为10）\n",
    "    activation : 'relu' or 'sigmoid'\n",
    "    weight_init_std : 指定权重的标准差（e.g. 0.01）\n",
    "        指定'relu'或'he'的情况下设定“He的初始值”\n",
    "        指定'sigmoid'或'xavier'的情况下设定“Xavier的初始值”\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 初始化权重\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 生成层\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"求损失函数\n",
    "        参数x是输入数据、t是教师标签\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"求梯度（数值微分）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"求梯度（误差反向传播法）\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 输入数据\n",
    "        t : 教师标签\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        具有各层的梯度的字典变量\n",
    "            grads['W1']、grads['W2']、...是各层的权重\n",
    "            grads['b1']、grads['b2']、...是各层的偏置\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 设定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用SimpleConvNet学习MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理花费时间较长的情况下减少数据 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2992300478350165\n",
      "=== epoch:1, train acc:0.265, test acc:0.272 ===\n",
      "train loss:2.297478775224145\n",
      "train loss:2.2926245363882094\n",
      "train loss:2.280445091975055\n",
      "train loss:2.2783119815355066\n",
      "train loss:2.261507821961983\n",
      "train loss:2.2556366185366867\n",
      "train loss:2.2363878850385874\n",
      "train loss:2.227229291332529\n",
      "train loss:2.175232747215168\n",
      "train loss:2.1669982875550637\n",
      "train loss:2.1174258365184535\n",
      "train loss:2.031172949536088\n",
      "train loss:2.0553322165945436\n",
      "train loss:1.9783190326547393\n",
      "train loss:1.8767618237000476\n",
      "train loss:1.782736201352219\n",
      "train loss:1.759021013029531\n",
      "train loss:1.6710611067470011\n",
      "train loss:1.6270141964706877\n",
      "train loss:1.5726790152361045\n",
      "train loss:1.468734960305297\n",
      "train loss:1.3816459468173858\n",
      "train loss:1.30741210178181\n",
      "train loss:1.1841478571962891\n",
      "train loss:1.171884734848853\n",
      "train loss:0.9816404196392426\n",
      "train loss:1.0462676109612064\n",
      "train loss:0.8268680598147281\n",
      "train loss:0.7760951516889435\n",
      "train loss:0.7397511110295595\n",
      "train loss:0.7554027734997897\n",
      "train loss:0.7328740627244511\n",
      "train loss:0.8284719801065878\n",
      "train loss:0.6954093369025268\n",
      "train loss:0.7394198698737998\n",
      "train loss:0.6262986626293964\n",
      "train loss:0.5361557152962796\n",
      "train loss:0.6288601637879139\n",
      "train loss:0.5151194866982761\n",
      "train loss:0.6445519651215054\n",
      "train loss:0.5379920993484982\n",
      "train loss:0.4904306135278825\n",
      "train loss:0.486382292415277\n",
      "train loss:0.7404657241459405\n",
      "train loss:0.5994346308775527\n",
      "train loss:0.7165696527809315\n",
      "train loss:0.4774559873087372\n",
      "train loss:0.6255983462621446\n",
      "train loss:0.29500566966274383\n",
      "train loss:0.3778646617660115\n",
      "=== epoch:2, train acc:0.823, test acc:0.809 ===\n",
      "train loss:0.6986427282663027\n",
      "train loss:0.5119526515551589\n",
      "train loss:0.5872884071183339\n",
      "train loss:0.6649539332578074\n",
      "train loss:0.4894089144850514\n",
      "train loss:0.47391979868987555\n",
      "train loss:0.3689153433280755\n",
      "train loss:0.36860979280863454\n",
      "train loss:0.6030876845283025\n",
      "train loss:0.5162419464421768\n",
      "train loss:0.43483983079675076\n",
      "train loss:0.4161127021600742\n",
      "train loss:0.2959898441840661\n",
      "train loss:0.3753274981155834\n",
      "train loss:0.4583634046580347\n",
      "train loss:0.37403605100654225\n",
      "train loss:0.49085047515306085\n",
      "train loss:0.5319231223322938\n",
      "train loss:0.6863873346547601\n",
      "train loss:0.38434404313967685\n",
      "train loss:0.5274754955245884\n",
      "train loss:0.3885534146276808\n",
      "train loss:0.4236157865159125\n",
      "train loss:0.41371449029378893\n",
      "train loss:0.3603587406582781\n",
      "train loss:0.3193939596525996\n",
      "train loss:0.43735135734931446\n",
      "train loss:0.39017149441157223\n",
      "train loss:0.40720267166580354\n",
      "train loss:0.6108667757440762\n",
      "train loss:0.34107690285994396\n",
      "train loss:0.42695139402313637\n",
      "train loss:0.46151414514636274\n",
      "train loss:0.2917853341723243\n",
      "train loss:0.4075244375074531\n",
      "train loss:0.28748226211436045\n",
      "train loss:0.337033544032618\n",
      "train loss:0.30739310302468836\n",
      "train loss:0.176482760075197\n",
      "train loss:0.31984787688205346\n",
      "train loss:0.3546207857629984\n",
      "train loss:0.4287832047214318\n",
      "train loss:0.2576889733861252\n",
      "train loss:0.23043757535813367\n",
      "train loss:0.42614880554110135\n",
      "train loss:0.3957108548695586\n",
      "train loss:0.33337975428097083\n",
      "train loss:0.47768327623514556\n",
      "train loss:0.33584301493865587\n",
      "train loss:0.22349663708816173\n",
      "=== epoch:3, train acc:0.887, test acc:0.863 ===\n",
      "train loss:0.41423715845086806\n",
      "train loss:0.26573301783904635\n",
      "train loss:0.2754470980023614\n",
      "train loss:0.32278372156977314\n",
      "train loss:0.2773713141028022\n",
      "train loss:0.2257494405904437\n",
      "train loss:0.3294758858055688\n",
      "train loss:0.2876122796556608\n",
      "train loss:0.2696137657409447\n",
      "train loss:0.24275644980961933\n",
      "train loss:0.43611969901117087\n",
      "train loss:0.4449103993901711\n",
      "train loss:0.2775477491177186\n",
      "train loss:0.3419515249577779\n",
      "train loss:0.23728765708918276\n",
      "train loss:0.3352402096450905\n",
      "train loss:0.335796311659066\n",
      "train loss:0.33049351176400343\n",
      "train loss:0.2058493406851965\n",
      "train loss:0.30149957335829297\n",
      "train loss:0.39769253451158115\n",
      "train loss:0.2976823300301815\n",
      "train loss:0.24754566696570252\n",
      "train loss:0.3415846734224332\n",
      "train loss:0.2145422117652859\n",
      "train loss:0.3209694167563695\n",
      "train loss:0.2941580020821476\n",
      "train loss:0.3061312425842553\n",
      "train loss:0.507334455032118\n",
      "train loss:0.27422521752509366\n",
      "train loss:0.27622896863532187\n",
      "train loss:0.49775431502144474\n",
      "train loss:0.15023791531304925\n",
      "train loss:0.25413519338794205\n",
      "train loss:0.1704823267608955\n",
      "train loss:0.2614352170720087\n",
      "train loss:0.18964275880994033\n",
      "train loss:0.21349321219031672\n",
      "train loss:0.16025631294438264\n",
      "train loss:0.3492855208324817\n",
      "train loss:0.18177741741396913\n",
      "train loss:0.18951974078730685\n",
      "train loss:0.42450086182921326\n",
      "train loss:0.22268319360516217\n",
      "train loss:0.18642008219647735\n",
      "train loss:0.398006030023733\n",
      "train loss:0.4289087775952139\n",
      "train loss:0.17738353475829186\n",
      "train loss:0.2423087966849962\n",
      "train loss:0.19173995159693258\n",
      "=== epoch:4, train acc:0.899, test acc:0.888 ===\n",
      "train loss:0.32241561082101833\n",
      "train loss:0.2835697579065699\n",
      "train loss:0.2233840184814456\n",
      "train loss:0.3656113425580008\n",
      "train loss:0.21137516725140112\n",
      "train loss:0.30144711795888\n",
      "train loss:0.19464321893504458\n",
      "train loss:0.19726798991294342\n",
      "train loss:0.21239591793047755\n",
      "train loss:0.252751486675167\n",
      "train loss:0.3143017774162364\n",
      "train loss:0.24076159299441638\n",
      "train loss:0.1927414011951713\n",
      "train loss:0.3198878809429051\n",
      "train loss:0.2931988420210903\n",
      "train loss:0.2597049884703927\n",
      "train loss:0.17199006057479188\n",
      "train loss:0.33803079728179175\n",
      "train loss:0.22986257754315742\n",
      "train loss:0.14362839812601857\n",
      "train loss:0.27283264034190224\n",
      "train loss:0.3426485820791762\n",
      "train loss:0.1596262303617585\n",
      "train loss:0.1842563252313088\n",
      "train loss:0.3848574654814556\n",
      "train loss:0.1297996707470388\n",
      "train loss:0.2838926989387595\n",
      "train loss:0.16005479407466214\n",
      "train loss:0.24616094101946623\n",
      "train loss:0.18157625763853272\n",
      "train loss:0.2519059375354375\n",
      "train loss:0.22277229539205534\n",
      "train loss:0.24750418964574178\n",
      "train loss:0.33399371211493234\n",
      "train loss:0.21459821992134173\n",
      "train loss:0.2614243426039964\n",
      "train loss:0.1450993399459477\n",
      "train loss:0.18381480026906719\n",
      "train loss:0.35119826950106486\n",
      "train loss:0.13230451528630938\n",
      "train loss:0.26827870452922525\n",
      "train loss:0.13790800643507248\n",
      "train loss:0.15177435525701852\n",
      "train loss:0.22677489827005257\n",
      "train loss:0.16350921631342238\n",
      "train loss:0.25741965082874907\n",
      "train loss:0.14333888244739384\n",
      "train loss:0.11607147203574757\n",
      "train loss:0.11404544765031809\n",
      "train loss:0.21947072403231188\n",
      "=== epoch:5, train acc:0.912, test acc:0.901 ===\n",
      "train loss:0.26114405780034233\n",
      "train loss:0.2970597715504571\n",
      "train loss:0.27816230391497904\n",
      "train loss:0.16341151501781212\n",
      "train loss:0.24435772989852647\n",
      "train loss:0.1633043083153089\n",
      "train loss:0.2225576128294004\n",
      "train loss:0.2908505608641635\n",
      "train loss:0.17749099253231865\n",
      "train loss:0.19366469283376833\n",
      "train loss:0.20137174273594996\n",
      "train loss:0.20199854349152013\n",
      "train loss:0.1533402272537298\n",
      "train loss:0.11492041293910779\n",
      "train loss:0.20728310968164165\n",
      "train loss:0.2531387291558618\n",
      "train loss:0.16346985906908895\n",
      "train loss:0.1728091489954328\n",
      "train loss:0.19172440753744213\n",
      "train loss:0.12494875216062606\n",
      "train loss:0.19910861735004723\n",
      "train loss:0.24221643087610964\n",
      "train loss:0.3174863491925268\n",
      "train loss:0.47790811342152745\n",
      "train loss:0.20342484396069999\n",
      "train loss:0.2039454401563545\n",
      "train loss:0.13976466827374495\n",
      "train loss:0.1928527170616442\n",
      "train loss:0.15939425754678385\n",
      "train loss:0.2137499324970472\n",
      "train loss:0.19643727940977143\n",
      "train loss:0.1795548514336642\n",
      "train loss:0.19211790285044292\n",
      "train loss:0.21889230594184939\n",
      "train loss:0.29200482815868345\n",
      "train loss:0.2520408809867257\n",
      "train loss:0.13301980107039096\n",
      "train loss:0.2180691188696061\n",
      "train loss:0.14118198068796697\n",
      "train loss:0.36606145598126316\n",
      "train loss:0.34072526344469223\n",
      "train loss:0.17447272894097224\n",
      "train loss:0.17932977878687711\n",
      "train loss:0.22213684953460974\n",
      "train loss:0.12909354546483282\n",
      "train loss:0.09460467473174519\n",
      "train loss:0.3488391003609733\n",
      "train loss:0.3152295455675902\n",
      "train loss:0.18849153187539702\n",
      "train loss:0.3212836023743417\n",
      "=== epoch:6, train acc:0.927, test acc:0.912 ===\n",
      "train loss:0.29105834769904043\n",
      "train loss:0.13917964114069825\n",
      "train loss:0.1618718639584748\n",
      "train loss:0.0706491834043187\n",
      "train loss:0.17713065382195192\n",
      "train loss:0.1631109884179044\n",
      "train loss:0.14320339224357936\n",
      "train loss:0.16001698064770398\n",
      "train loss:0.12014741467780381\n",
      "train loss:0.24932491490644407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.07485454666375162\n",
      "train loss:0.1431342576298811\n",
      "train loss:0.09725016732456121\n",
      "train loss:0.09762001904759217\n",
      "train loss:0.12185368390339552\n",
      "train loss:0.292604332465398\n",
      "train loss:0.11990248733001099\n",
      "train loss:0.2800513518756605\n",
      "train loss:0.17122492433338923\n",
      "train loss:0.12498334446827485\n",
      "train loss:0.11416885140142093\n",
      "train loss:0.22626135300281397\n",
      "train loss:0.22489784922780104\n",
      "train loss:0.1298461397680505\n",
      "train loss:0.20663933317832264\n",
      "train loss:0.12067144362915817\n",
      "train loss:0.16508883666900775\n",
      "train loss:0.14188706524907949\n",
      "train loss:0.1705037051056344\n",
      "train loss:0.20230376424076174\n",
      "train loss:0.17869949637996851\n",
      "train loss:0.10774703348080303\n",
      "train loss:0.13743146274572285\n",
      "train loss:0.13651908941687238\n",
      "train loss:0.2172731614342879\n",
      "train loss:0.12536733180361814\n",
      "train loss:0.16566885690549626\n",
      "train loss:0.18391366256101307\n",
      "train loss:0.1821362400707504\n",
      "train loss:0.2669694190266414\n",
      "train loss:0.17983493342747792\n",
      "train loss:0.11938038183772273\n",
      "train loss:0.08436424953106032\n",
      "train loss:0.19103580688935642\n",
      "train loss:0.12837250924023028\n",
      "train loss:0.10169228566452616\n",
      "train loss:0.21091967645259946\n",
      "train loss:0.19221279692159476\n",
      "train loss:0.23233499701207602\n",
      "train loss:0.21190879164588458\n",
      "=== epoch:7, train acc:0.944, test acc:0.926 ===\n",
      "train loss:0.26304608408135033\n",
      "train loss:0.10682769356602752\n",
      "train loss:0.1467821929750961\n",
      "train loss:0.08040929120346484\n",
      "train loss:0.22695052629839843\n",
      "train loss:0.08720913495716537\n",
      "train loss:0.15369958573065826\n",
      "train loss:0.1608699429629759\n",
      "train loss:0.13439776826059935\n",
      "train loss:0.07910764586270556\n",
      "train loss:0.24218908547233275\n",
      "train loss:0.1646881767363549\n",
      "train loss:0.14475666200210566\n",
      "train loss:0.15707543086020617\n",
      "train loss:0.08494094186140218\n",
      "train loss:0.07526875477750962\n",
      "train loss:0.10525945246344921\n",
      "train loss:0.15507398621815002\n",
      "train loss:0.13439791114645278\n",
      "train loss:0.19133041082811142\n",
      "train loss:0.15326371429653346\n",
      "train loss:0.17357388198092022\n",
      "train loss:0.1410387196699198\n",
      "train loss:0.11027980890084742\n",
      "train loss:0.12176135671833893\n",
      "train loss:0.08591283687845504\n",
      "train loss:0.17553293081056445\n",
      "train loss:0.1510392396401634\n",
      "train loss:0.08730367966294664\n",
      "train loss:0.10740103913847218\n",
      "train loss:0.07934557748216929\n",
      "train loss:0.16486360644313955\n",
      "train loss:0.20154670235987793\n",
      "train loss:0.23075409388335744\n",
      "train loss:0.0859504063129739\n",
      "train loss:0.1227762135182717\n",
      "train loss:0.13623383255937413\n",
      "train loss:0.1333457398050901\n",
      "train loss:0.08910866074159987\n",
      "train loss:0.18983379253705038\n",
      "train loss:0.12154198557598538\n",
      "train loss:0.14952906602431892\n",
      "train loss:0.14569508687202437\n",
      "train loss:0.12334753306836271\n",
      "train loss:0.06129291824698645\n",
      "train loss:0.1356366028623318\n",
      "train loss:0.13872107764888794\n",
      "train loss:0.07712647756068314\n",
      "train loss:0.05620352330387635\n",
      "train loss:0.237286361035127\n",
      "=== epoch:8, train acc:0.951, test acc:0.929 ===\n",
      "train loss:0.21936284778613063\n",
      "train loss:0.044333572501490205\n",
      "train loss:0.07207759419843368\n",
      "train loss:0.16076832464985194\n",
      "train loss:0.10650679348550356\n",
      "train loss:0.173133685122718\n",
      "train loss:0.13389874489594175\n",
      "train loss:0.1285495520192325\n",
      "train loss:0.11566600126534796\n",
      "train loss:0.12183200917182749\n",
      "train loss:0.23997117765279408\n",
      "train loss:0.09079556920138801\n",
      "train loss:0.0878063557878962\n",
      "train loss:0.18240279705275525\n",
      "train loss:0.1754239517584301\n",
      "train loss:0.25045499525395093\n",
      "train loss:0.10681770841492252\n",
      "train loss:0.11124838087888225\n",
      "train loss:0.04661100737443311\n",
      "train loss:0.08117429045359936\n",
      "train loss:0.18684931653500825\n",
      "train loss:0.08342562029548593\n",
      "train loss:0.11092881055857302\n",
      "train loss:0.11907768073921454\n",
      "train loss:0.11087331443462402\n",
      "train loss:0.1476610034275919\n",
      "train loss:0.15495541776176688\n",
      "train loss:0.08408944474057645\n",
      "train loss:0.08040946293527146\n",
      "train loss:0.21920145262630616\n",
      "train loss:0.1638735431020557\n",
      "train loss:0.13567970557169493\n",
      "train loss:0.0918606810814412\n",
      "train loss:0.10411446169042722\n",
      "train loss:0.12198916283725796\n",
      "train loss:0.056038227432127045\n",
      "train loss:0.0835554766113148\n",
      "train loss:0.20233764058929585\n",
      "train loss:0.12085148111197613\n",
      "train loss:0.13441713861861848\n",
      "train loss:0.09186189534519597\n",
      "train loss:0.06152496435715378\n",
      "train loss:0.05049222561380406\n",
      "train loss:0.11788011186662355\n",
      "train loss:0.12258904855512251\n",
      "train loss:0.1316216048528337\n",
      "train loss:0.14795021105327716\n",
      "train loss:0.08594042972371434\n",
      "train loss:0.10351478104860931\n",
      "train loss:0.13298855657706282\n",
      "=== epoch:9, train acc:0.962, test acc:0.937 ===\n",
      "train loss:0.047075473638133004\n",
      "train loss:0.0575584539413806\n",
      "train loss:0.05215469471506802\n",
      "train loss:0.1286777023269111\n",
      "train loss:0.08882632558338269\n",
      "train loss:0.11262000594801283\n",
      "train loss:0.11788942270374339\n",
      "train loss:0.07212972838315196\n",
      "train loss:0.18165557256251894\n",
      "train loss:0.07423920776952171\n",
      "train loss:0.09596062350625033\n",
      "train loss:0.10853258615823716\n",
      "train loss:0.05559643691288467\n",
      "train loss:0.08530527635903098\n",
      "train loss:0.11391611050439404\n",
      "train loss:0.06734193566771335\n",
      "train loss:0.09784284969740467\n",
      "train loss:0.09632926864899036\n",
      "train loss:0.09465838641511315\n",
      "train loss:0.06970675257623137\n",
      "train loss:0.07714770341665594\n",
      "train loss:0.12211890480337491\n",
      "train loss:0.06430105689045593\n",
      "train loss:0.06821099403009445\n",
      "train loss:0.10672089326592793\n",
      "train loss:0.14955261470529613\n",
      "train loss:0.0953003403143872\n",
      "train loss:0.16220070950304624\n",
      "train loss:0.07512538089129571\n",
      "train loss:0.12589568990968836\n",
      "train loss:0.2727137638603795\n",
      "train loss:0.1311889667098289\n",
      "train loss:0.11268895271678447\n",
      "train loss:0.05798852287901521\n",
      "train loss:0.09138502165818303\n",
      "train loss:0.12579452281909007\n",
      "train loss:0.10958285341530695\n",
      "train loss:0.10903336924582531\n",
      "train loss:0.10172214329248554\n",
      "train loss:0.07608114257352817\n",
      "train loss:0.08023993602314493\n",
      "train loss:0.06519959390520973\n",
      "train loss:0.07300416431882578\n",
      "train loss:0.07114668174711582\n",
      "train loss:0.13045015579789632\n",
      "train loss:0.1600764659132599\n",
      "train loss:0.07369097328821751\n",
      "train loss:0.06543597663548634\n",
      "train loss:0.11240269639683585\n",
      "train loss:0.07317626070767196\n",
      "=== epoch:10, train acc:0.955, test acc:0.933 ===\n",
      "train loss:0.06797705752002116\n",
      "train loss:0.07814778787457718\n",
      "train loss:0.08397925254016549\n",
      "train loss:0.05005023118541351\n",
      "train loss:0.06756793246517369\n",
      "train loss:0.04455467787252456\n",
      "train loss:0.07857388528965244\n",
      "train loss:0.06419276583741981\n",
      "train loss:0.1508509218842341\n",
      "train loss:0.15117931661719056\n",
      "train loss:0.07854396238290116\n",
      "train loss:0.09824269661886133\n",
      "train loss:0.07387609534374627\n",
      "train loss:0.07220079663146092\n",
      "train loss:0.08933593706752363\n",
      "train loss:0.08352157859921688\n",
      "train loss:0.08764534658780104\n",
      "train loss:0.06517082381968618\n",
      "train loss:0.122976617156556\n",
      "train loss:0.09090929599851359\n",
      "train loss:0.059122782536107366\n",
      "train loss:0.07927973665759845\n",
      "train loss:0.05875800163908279\n",
      "train loss:0.0838604814844693\n",
      "train loss:0.09272854483291773\n",
      "train loss:0.10368242393258224\n",
      "train loss:0.1300254753352658\n",
      "train loss:0.07798641781697206\n",
      "train loss:0.07331533216375791\n",
      "train loss:0.19104959133221452\n",
      "train loss:0.094981618152622\n",
      "train loss:0.10597936451149376\n",
      "train loss:0.09743584596500332\n",
      "train loss:0.18068079883843158\n",
      "train loss:0.018447871069360338\n",
      "train loss:0.05822147488029154\n",
      "train loss:0.12025456475849189\n",
      "train loss:0.08602229119679695\n",
      "train loss:0.056234394203263746\n",
      "train loss:0.03937793694694781\n",
      "train loss:0.13624021757541693\n",
      "train loss:0.10598180446998365\n",
      "train loss:0.05255154892847717\n",
      "train loss:0.1560386335622554\n",
      "train loss:0.06515851713653195\n",
      "train loss:0.11851275735739959\n",
      "train loss:0.12603216324633562\n",
      "train loss:0.07525452265290401\n",
      "train loss:0.11619856885397085\n",
      "train loss:0.047748911787766544\n",
      "=== epoch:11, train acc:0.968, test acc:0.95 ===\n",
      "train loss:0.052209051670989035\n",
      "train loss:0.0846602544509779\n",
      "train loss:0.06639211378556588\n",
      "train loss:0.12352070350138185\n",
      "train loss:0.11172475577574392\n",
      "train loss:0.1039025099215206\n",
      "train loss:0.02989168174802075\n",
      "train loss:0.08568235372166472\n",
      "train loss:0.1059247281690357\n",
      "train loss:0.08401338214011474\n",
      "train loss:0.08320175369736621\n",
      "train loss:0.057149243892614915\n",
      "train loss:0.06384630359917491\n",
      "train loss:0.05215785613469131\n",
      "train loss:0.04815877312214344\n",
      "train loss:0.032554533253907864\n",
      "train loss:0.07612471987210827\n",
      "train loss:0.0813734081457406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.12764560790646873\n",
      "train loss:0.09328199175813462\n",
      "train loss:0.08324459751325791\n",
      "train loss:0.11355194443311398\n",
      "train loss:0.02900224840554345\n",
      "train loss:0.05667423585006597\n",
      "train loss:0.03867326341277692\n",
      "train loss:0.029477010020696935\n",
      "train loss:0.06891577296353915\n",
      "train loss:0.04950959987044605\n",
      "train loss:0.05119835194188518\n",
      "train loss:0.041241521625624146\n",
      "train loss:0.049535021680630544\n",
      "train loss:0.08823171949407728\n",
      "train loss:0.04448757883229639\n",
      "train loss:0.07984999644616567\n",
      "train loss:0.0697634119103808\n",
      "train loss:0.030174260545823516\n",
      "train loss:0.16231362795528942\n",
      "train loss:0.07961657121812586\n",
      "train loss:0.04046635348197977\n",
      "train loss:0.08043561284488282\n",
      "train loss:0.06718295968661678\n",
      "train loss:0.1266187217045595\n",
      "train loss:0.09235639125009609\n",
      "train loss:0.06343193642037502\n",
      "train loss:0.09056818248353381\n",
      "train loss:0.06609614504819573\n",
      "train loss:0.04964209333577565\n",
      "train loss:0.05243003190291301\n",
      "train loss:0.04912727209677506\n",
      "train loss:0.059319201438432216\n",
      "=== epoch:12, train acc:0.972, test acc:0.956 ===\n",
      "train loss:0.09472104599585844\n",
      "train loss:0.036521918670114964\n",
      "train loss:0.1593342449839865\n",
      "train loss:0.02437932792739629\n",
      "train loss:0.07458683229845703\n",
      "train loss:0.03427264643776362\n",
      "train loss:0.05226196839439605\n",
      "train loss:0.07436013312671118\n",
      "train loss:0.025054870397338627\n",
      "train loss:0.03397528872450868\n",
      "train loss:0.050031902544735045\n",
      "train loss:0.03287482137064962\n",
      "train loss:0.052223019519291654\n",
      "train loss:0.08230206878303942\n",
      "train loss:0.06635793426584825\n",
      "train loss:0.05343437014605054\n",
      "train loss:0.07799833551481529\n",
      "train loss:0.09643353619434773\n",
      "train loss:0.04881901657228704\n",
      "train loss:0.03420500220168753\n",
      "train loss:0.03110173516696016\n",
      "train loss:0.052871817012559136\n",
      "train loss:0.06744202627932007\n",
      "train loss:0.08915773677792957\n",
      "train loss:0.11861217753532959\n",
      "train loss:0.055792629072890405\n",
      "train loss:0.08236424634387993\n",
      "train loss:0.1035440021979191\n",
      "train loss:0.06707815903151472\n",
      "train loss:0.055239703898015365\n",
      "train loss:0.16689320028801408\n",
      "train loss:0.04690688380471595\n",
      "train loss:0.0827291840474168\n",
      "train loss:0.11217115082032525\n",
      "train loss:0.15507304670820307\n",
      "train loss:0.1767305666178973\n",
      "train loss:0.0673158679646981\n",
      "train loss:0.05499683995432844\n",
      "train loss:0.21933480879255235\n",
      "train loss:0.1136019932358315\n",
      "train loss:0.09969967722236218\n",
      "train loss:0.09017793259682659\n",
      "train loss:0.05604200535648902\n",
      "train loss:0.07662168775426553\n",
      "train loss:0.1465174088509926\n",
      "train loss:0.044693862243678\n",
      "train loss:0.11252504301972305\n",
      "train loss:0.06427688658668877\n",
      "train loss:0.06364539790066606\n",
      "train loss:0.028755769772093254\n",
      "=== epoch:13, train acc:0.969, test acc:0.949 ===\n",
      "train loss:0.0837075231621619\n",
      "train loss:0.05551627598276869\n",
      "train loss:0.07487474141199542\n",
      "train loss:0.10748316018996708\n",
      "train loss:0.03339545736155334\n",
      "train loss:0.0406740172799908\n",
      "train loss:0.07203616862833911\n",
      "train loss:0.020039917233003145\n",
      "train loss:0.054085846340887686\n",
      "train loss:0.06001773081273257\n",
      "train loss:0.043664390723146755\n",
      "train loss:0.032117531030343684\n",
      "train loss:0.11410325056273353\n",
      "train loss:0.0410931154785246\n",
      "train loss:0.03946571549416547\n",
      "train loss:0.04867627502982448\n",
      "train loss:0.042586044023275\n",
      "train loss:0.09358737503323004\n",
      "train loss:0.038966172472614725\n",
      "train loss:0.04250303857116165\n",
      "train loss:0.06466996955400207\n",
      "train loss:0.06042285775360515\n",
      "train loss:0.05726221296794105\n",
      "train loss:0.06301433321316308\n",
      "train loss:0.12082785083475807\n",
      "train loss:0.014734110630608581\n",
      "train loss:0.02579480435448295\n",
      "train loss:0.0769352069428437\n",
      "train loss:0.07536839988999379\n",
      "train loss:0.03338718595265646\n",
      "train loss:0.021657185569164537\n",
      "train loss:0.0689106306964137\n",
      "train loss:0.05682592948378378\n",
      "train loss:0.06872162190281933\n",
      "train loss:0.053876434645023986\n",
      "train loss:0.04012981568722173\n",
      "train loss:0.06902834843253115\n",
      "train loss:0.04483959682848208\n",
      "train loss:0.03717575220422112\n",
      "train loss:0.055685303488174374\n",
      "train loss:0.0153410010281147\n",
      "train loss:0.047876236644298305\n",
      "train loss:0.0626207657389514\n",
      "train loss:0.025119408468197078\n",
      "train loss:0.08529794075768966\n",
      "train loss:0.031644494646842866\n",
      "train loss:0.05504546568564657\n",
      "train loss:0.01766097454892297\n",
      "train loss:0.05207143057613191\n",
      "train loss:0.023848560291632893\n",
      "=== epoch:14, train acc:0.976, test acc:0.951 ===\n",
      "train loss:0.04337773397344257\n",
      "train loss:0.07076078188331197\n",
      "train loss:0.034334237229908776\n",
      "train loss:0.04562943914185217\n",
      "train loss:0.15937775376402255\n",
      "train loss:0.06986406147785716\n",
      "train loss:0.06346216920207154\n",
      "train loss:0.04117211232299617\n",
      "train loss:0.015248034032587397\n",
      "train loss:0.03877321276904056\n",
      "train loss:0.08130887469141561\n",
      "train loss:0.052565106951888235\n",
      "train loss:0.023931643884738535\n",
      "train loss:0.10194344118758343\n",
      "train loss:0.022990985021119\n",
      "train loss:0.08861463568904805\n",
      "train loss:0.03109288511418817\n",
      "train loss:0.026279564868845307\n",
      "train loss:0.06388986832885975\n",
      "train loss:0.019972162284508262\n",
      "train loss:0.03935776780978183\n",
      "train loss:0.028027174127730082\n",
      "train loss:0.027455125703147322\n",
      "train loss:0.016541936585043007\n",
      "train loss:0.02883324211890369\n",
      "train loss:0.03690910626965736\n",
      "train loss:0.03292017856378168\n",
      "train loss:0.06588566301211184\n",
      "train loss:0.0442418459804989\n",
      "train loss:0.07906950648593356\n",
      "train loss:0.0290021794604221\n",
      "train loss:0.03486616120580608\n",
      "train loss:0.05198339748984199\n",
      "train loss:0.034351164374427465\n",
      "train loss:0.023159588191366366\n",
      "train loss:0.0427931273139696\n",
      "train loss:0.014566187459108619\n",
      "train loss:0.049017914908951556\n",
      "train loss:0.029237492560455554\n",
      "train loss:0.12255123288916982\n",
      "train loss:0.02986446469466522\n",
      "train loss:0.0369830319749185\n",
      "train loss:0.040231827030073684\n",
      "train loss:0.015973290463325986\n",
      "train loss:0.0699754257309134\n",
      "train loss:0.01739814308161847\n",
      "train loss:0.015471844491994543\n",
      "train loss:0.03609858854520516\n",
      "train loss:0.029938746404777415\n",
      "train loss:0.0347042949475491\n",
      "=== epoch:15, train acc:0.983, test acc:0.964 ===\n",
      "train loss:0.04181522478891221\n",
      "train loss:0.025876150904349426\n",
      "train loss:0.018909303167255215\n",
      "train loss:0.026184525439847443\n",
      "train loss:0.050720600394873\n",
      "train loss:0.06128764415472486\n",
      "train loss:0.030260295253010792\n",
      "train loss:0.04370142322555171\n",
      "train loss:0.03159429089830689\n",
      "train loss:0.038445025983569324\n",
      "train loss:0.07863411103826838\n",
      "train loss:0.03256837513763727\n",
      "train loss:0.03785146631640227\n",
      "train loss:0.027221784890707288\n",
      "train loss:0.03944020136078397\n",
      "train loss:0.007652619061855747\n",
      "train loss:0.030161108869912304\n",
      "train loss:0.02969413837488268\n",
      "train loss:0.0685718603251003\n",
      "train loss:0.05767781994000242\n",
      "train loss:0.07798821001611682\n",
      "train loss:0.04967374572649317\n",
      "train loss:0.03056709687891923\n",
      "train loss:0.02801561715201708\n",
      "train loss:0.04173209655073384\n",
      "train loss:0.06107366996913157\n",
      "train loss:0.0747761179827187\n",
      "train loss:0.06367694739179341\n",
      "train loss:0.03499631360317071\n",
      "train loss:0.020955735834633313\n",
      "train loss:0.033587734911681384\n",
      "train loss:0.03517824850880944\n",
      "train loss:0.03794763018404312\n",
      "train loss:0.02290908698419335\n",
      "train loss:0.03318625443948663\n",
      "train loss:0.036400699895719354\n",
      "train loss:0.010608027810212925\n",
      "train loss:0.05387621818658994\n",
      "train loss:0.039274515521836056\n",
      "train loss:0.023560943653438606\n",
      "train loss:0.02479374676191255\n",
      "train loss:0.019039387600765425\n",
      "train loss:0.02159044110459495\n",
      "train loss:0.03416158293649118\n",
      "train loss:0.04179570845129241\n",
      "train loss:0.031090846891388343\n",
      "train loss:0.029715230404487963\n",
      "train loss:0.015209161029697549\n",
      "train loss:0.014926108788321119\n",
      "train loss:0.07533813356593616\n",
      "=== epoch:16, train acc:0.985, test acc:0.962 ===\n",
      "train loss:0.03138180455490554\n",
      "train loss:0.047839825610969756\n",
      "train loss:0.02870842567521654\n",
      "train loss:0.033712667348708855\n",
      "train loss:0.02075822465050781\n",
      "train loss:0.06214448219738254\n",
      "train loss:0.015360352785629831\n",
      "train loss:0.06626692649642237\n",
      "train loss:0.012787535210203087\n",
      "train loss:0.04642549254230085\n",
      "train loss:0.060344029509590884\n",
      "train loss:0.020726627165681334\n",
      "train loss:0.022338308496918393\n",
      "train loss:0.0301848499366435\n",
      "train loss:0.028908817874595427\n",
      "train loss:0.04679054831353274\n",
      "train loss:0.028466540335469116\n",
      "train loss:0.04454086057626041\n",
      "train loss:0.032649631795111586\n",
      "train loss:0.02113164996243319\n",
      "train loss:0.01631853738096814\n",
      "train loss:0.01592867856567585\n",
      "train loss:0.02050835653818249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:0.024148729777933264\n",
      "train loss:0.040140256153693966\n",
      "train loss:0.023202494791706777\n",
      "train loss:0.026597723010730066\n",
      "train loss:0.04195252602388421\n",
      "train loss:0.0703758957592107\n",
      "train loss:0.03137434926699073\n",
      "train loss:0.02744838811121342\n",
      "train loss:0.10389592446952849\n",
      "train loss:0.027769137898517017\n",
      "train loss:0.061164001914293074\n",
      "train loss:0.033914565085341096\n",
      "train loss:0.050802041012661246\n",
      "train loss:0.047793585799278435\n",
      "train loss:0.039193103349047916\n",
      "train loss:0.024204256472559815\n",
      "train loss:0.01159177316639982\n",
      "train loss:0.03434570392068178\n",
      "train loss:0.021245528566721253\n",
      "train loss:0.0330004277887549\n",
      "train loss:0.056986400511746006\n",
      "train loss:0.027775843947667887\n",
      "train loss:0.026868842205116854\n",
      "train loss:0.039152096571661965\n",
      "train loss:0.11077630323792839\n",
      "train loss:0.01886478215673272\n",
      "train loss:0.02887923832690829\n",
      "=== epoch:17, train acc:0.988, test acc:0.958 ===\n",
      "train loss:0.05294490716356076\n",
      "train loss:0.012874107048408148\n",
      "train loss:0.07311669254335733\n",
      "train loss:0.04150991810849338\n",
      "train loss:0.016602177685829007\n",
      "train loss:0.04043081485489815\n",
      "train loss:0.030466075305189966\n",
      "train loss:0.025976423709744477\n",
      "train loss:0.013581396387569934\n",
      "train loss:0.023794276517673782\n",
      "train loss:0.05359282090964897\n",
      "train loss:0.014763766864954161\n",
      "train loss:0.08300829440342819\n",
      "train loss:0.009458584951112873\n",
      "train loss:0.11837999779871547\n",
      "train loss:0.014059790244677562\n",
      "train loss:0.024076679105843907\n",
      "train loss:0.02996187941092191\n",
      "train loss:0.052525193754583505\n",
      "train loss:0.022785299028170943\n",
      "train loss:0.018806117706656487\n",
      "train loss:0.026446836366122616\n",
      "train loss:0.05663222474480376\n",
      "train loss:0.03432167723910955\n",
      "train loss:0.043434032002462715\n",
      "train loss:0.012023160550664838\n",
      "train loss:0.03214736125285066\n",
      "train loss:0.0168778554675805\n",
      "train loss:0.02527917857527992\n",
      "train loss:0.011644207663091634\n",
      "train loss:0.03730234355378536\n",
      "train loss:0.01478140388593311\n",
      "train loss:0.06951392524066242\n",
      "train loss:0.03177036831527611\n",
      "train loss:0.044720474820836235\n",
      "train loss:0.033508164123721144\n",
      "train loss:0.010607568015631966\n",
      "train loss:0.0722929758440051\n",
      "train loss:0.02620871035729131\n",
      "train loss:0.048477678470156375\n",
      "train loss:0.014225607100053\n",
      "train loss:0.01961815177405661\n",
      "train loss:0.040163489298818626\n",
      "train loss:0.04509542208207872\n",
      "train loss:0.020452478178379866\n",
      "train loss:0.022003149966241843\n",
      "train loss:0.015405304125854136\n",
      "train loss:0.025998065978619474\n",
      "train loss:0.013897120754584897\n",
      "train loss:0.04810093959391295\n",
      "=== epoch:18, train acc:0.988, test acc:0.962 ===\n",
      "train loss:0.032965426239612315\n",
      "train loss:0.01616824937929244\n",
      "train loss:0.013646704652433861\n",
      "train loss:0.031428498198585245\n",
      "train loss:0.028013399573454455\n",
      "train loss:0.015605847853342214\n",
      "train loss:0.0635252951771776\n",
      "train loss:0.012007816798845598\n",
      "train loss:0.02250498347372944\n",
      "train loss:0.016545383656515672\n",
      "train loss:0.052431654025771046\n",
      "train loss:0.025824066262445485\n",
      "train loss:0.034907484273013746\n",
      "train loss:0.0191063520273379\n",
      "train loss:0.012721015655847127\n",
      "train loss:0.03230492840354897\n",
      "train loss:0.03863599998333699\n",
      "train loss:0.014728607777017318\n",
      "train loss:0.0253998296699685\n",
      "train loss:0.010796209014844278\n",
      "train loss:0.038710598204835654\n",
      "train loss:0.01549056030214364\n",
      "train loss:0.008648230703192145\n",
      "train loss:0.023571669387595712\n",
      "train loss:0.041087252354489186\n",
      "train loss:0.03669988884257698\n",
      "train loss:0.004816797444423179\n",
      "train loss:0.008872663645986375\n",
      "train loss:0.039381755355411635\n",
      "train loss:0.016053915315375412\n",
      "train loss:0.00835719542385155\n",
      "train loss:0.014365402733835768\n",
      "train loss:0.017868260558294463\n",
      "train loss:0.04666946087958321\n",
      "train loss:0.026779317753834405\n",
      "train loss:0.03440686537053758\n",
      "train loss:0.044283271914377635\n",
      "train loss:0.01534017873796943\n",
      "train loss:0.03390625016063784\n",
      "train loss:0.01676861571629646\n",
      "train loss:0.015129792961475808\n",
      "train loss:0.006738717508107665\n",
      "train loss:0.024697658430240653\n",
      "train loss:0.01345540385936309\n",
      "train loss:0.02335545009638014\n",
      "train loss:0.024010888351106714\n",
      "train loss:0.02566554852323068\n",
      "train loss:0.023247828518096262\n",
      "train loss:0.02656930923090712\n",
      "train loss:0.008431615062625129\n",
      "=== epoch:19, train acc:0.991, test acc:0.96 ===\n",
      "train loss:0.01047070048322879\n",
      "train loss:0.01965507618045472\n",
      "train loss:0.02390206161166056\n",
      "train loss:0.040644725781164424\n",
      "train loss:0.05014087471121364\n",
      "train loss:0.01572881008305065\n",
      "train loss:0.01287416385137411\n",
      "train loss:0.014857396072719124\n",
      "train loss:0.0447137872486617\n",
      "train loss:0.01823795055533664\n",
      "train loss:0.0156004355055615\n",
      "train loss:0.012951360296286412\n",
      "train loss:0.01624692740999485\n",
      "train loss:0.038207814006050195\n",
      "train loss:0.043296711532228846\n",
      "train loss:0.020330226193437717\n",
      "train loss:0.026057100647452182\n",
      "train loss:0.019023503120275686\n",
      "train loss:0.04473064979829096\n",
      "train loss:0.04198974129843096\n",
      "train loss:0.03655671129794298\n",
      "train loss:0.011043421552444707\n",
      "train loss:0.010786155848546081\n",
      "train loss:0.019314941697807234\n",
      "train loss:0.022394991416559408\n",
      "train loss:0.029575874175075095\n",
      "train loss:0.017822798058100244\n",
      "train loss:0.06079175946422957\n",
      "train loss:0.03558019494560572\n",
      "train loss:0.014227975190088536\n",
      "train loss:0.020962793306576577\n",
      "train loss:0.01355877535137407\n",
      "train loss:0.007667862059736803\n",
      "train loss:0.018723679814377475\n",
      "train loss:0.03791016298989316\n",
      "train loss:0.02583893166833576\n",
      "train loss:0.02719983969829858\n",
      "train loss:0.03524382981568101\n",
      "train loss:0.025115166598295855\n",
      "train loss:0.03791466165166773\n",
      "train loss:0.01102994428760163\n",
      "train loss:0.015195407570704565\n",
      "train loss:0.011910389020271726\n",
      "train loss:0.009796512430696848\n",
      "train loss:0.04968720593478032\n",
      "train loss:0.010017709902278939\n",
      "train loss:0.010151428483562638\n",
      "train loss:0.024725924771216063\n",
      "train loss:0.03567193305116395\n",
      "train loss:0.01088015128551923\n",
      "=== epoch:20, train acc:0.991, test acc:0.968 ===\n",
      "train loss:0.01616851913820905\n",
      "train loss:0.026994766230204855\n",
      "train loss:0.010821870499000054\n",
      "train loss:0.04230840861777569\n",
      "train loss:0.008741668143753533\n",
      "train loss:0.02415933096848996\n",
      "train loss:0.050402291272346245\n",
      "train loss:0.02348552344459958\n",
      "train loss:0.012889465364732683\n",
      "train loss:0.017892409837696955\n",
      "train loss:0.018876667437152377\n",
      "train loss:0.02013475216177988\n",
      "train loss:0.0236109526386354\n",
      "train loss:0.01966786400768371\n",
      "train loss:0.02516031998442103\n",
      "train loss:0.012141540741538564\n",
      "train loss:0.07992287005073005\n",
      "train loss:0.014832271240079431\n",
      "train loss:0.01997184070085454\n",
      "train loss:0.027623232401901694\n",
      "train loss:0.04162824747195692\n",
      "train loss:0.015600325736228365\n",
      "train loss:0.015776177148364177\n",
      "train loss:0.014601371876769097\n",
      "train loss:0.033071025635931284\n",
      "train loss:0.014091233678522296\n",
      "train loss:0.016215168554896405\n",
      "train loss:0.008028359657923406\n",
      "train loss:0.01329103801434573\n",
      "train loss:0.022091186423592372\n",
      "train loss:0.01026774526563057\n",
      "train loss:0.021909325996592992\n",
      "train loss:0.007975030838651635\n",
      "train loss:0.033516234236353803\n",
      "train loss:0.015572974385921964\n",
      "train loss:0.011554086719580097\n",
      "train loss:0.011739191645353298\n",
      "train loss:0.030976367301902456\n",
      "train loss:0.013194194675933411\n",
      "train loss:0.005972405042698173\n",
      "train loss:0.020373215694126506\n",
      "train loss:0.01671050055936046\n",
      "train loss:0.004033302990544586\n",
      "train loss:0.028023817041231264\n",
      "train loss:0.02178202954537389\n",
      "train loss:0.06931590247798904\n",
      "train loss:0.011431658803276422\n",
      "train loss:0.00634410582998054\n",
      "train loss:0.010021843100247287\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.964\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN的可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualize_filter.py\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 随机进行初始化后的权重\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学习后的权重\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
